{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225a2a75-9dbf-4e6a-8388-d77f463dab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting filelock (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (73.0.1)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2024.7.4)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:19\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading filelock-3.20.2-py3-none-any.whl (16 kB)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, typing_extensions, tqdm, threadpoolctl, sympy, scipy, safetensors, regex, networkx, joblib, hf-xet, fsspec, filelock, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/19\u001b[0m [sentence-transformers]sformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.2 fsspec-2025.12.0 hf-xet-1.2.0 huggingface-hub-0.36.0 joblib-1.5.3 mpmath-1.3.0 networkx-3.6.1 regex-2025.11.3 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.16.3 sentence-transformers-5.2.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.9.1 tqdm-4.67.1 transformers-4.57.3 typing_extensions-4.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4879ca3e-b846-475a-8fac-0b0e6787c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING EMBEDDINGS FROM CHUNKS.JSON\n",
      "============================================================\n",
      "\n",
      "[1/3] Loading chunks from JSON...\n",
      "Loading chunks from chunks.json...\n",
      "Loaded 3827 chunks\n",
      "\n",
      "First chunk structure:\n",
      "{\n",
      "  \"video\": \"Ask_Dr_Tarek__Session_1.txt\",\n",
      "  \"chunk_id\": 0,\n",
      "  \"text\": \"so i have some questions that were sent i'm going to go over the questions i have them here i'll go over them one by one inshallah uh some of them will be short there's a couple of repeats uh one or two need a little bit of expl...\n",
      "\n",
      "[2/3] Creating embeddings...\n",
      "\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "\n",
      "Creating embeddings for 3827 chunks...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████| 120/120 [00:08<00:00, 14.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/3] Saving database...\n",
      "\n",
      "============================================================\n",
      "DATABASE SAVED SUCCESSFULLY!\n",
      "============================================================\n",
      "Output file: embeddings_database.pkl\n",
      "Total chunks: 3827\n",
      "Embedding dimensions: 384\n",
      "Approximate file size: 7.63 MB\n",
      "============================================================\n",
      "\n",
      "✓ PROCESSING COMPLETE!\n",
      "\n",
      "You can now use 'embeddings_database.pkl' for semantic search\n",
      "\n",
      "============================================================\n",
      "SAMPLE CHUNK:\n",
      "============================================================\n",
      "Text preview: so i have some questions that were sent i'm going to go over the questions i have them here i'll go over them one by one inshallah uh some of them will be short there's a couple of repeats uh one or t...\n",
      "Embedding shape: (384,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# Configuration\n",
    "CHUNKS_FILE = \"chunks.json\"  # Your existing chunks file\n",
    "OUTPUT_FILE = \"embeddings_database.pkl\"\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'  # Fast and efficient model\n",
    "\n",
    "def load_chunks(chunks_file):\n",
    "    \"\"\"Load chunks from your JSON file\"\"\"\n",
    "    print(f\"Loading chunks from {chunks_file}...\")\n",
    "    \n",
    "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(chunks)} chunks\")\n",
    "    \n",
    "    # Display structure of first chunk to understand the format\n",
    "    if chunks:\n",
    "        print(\"\\nFirst chunk structure:\")\n",
    "        print(json.dumps(chunks[0], indent=2)[:300] + \"...\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_embeddings(chunks, model_name=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Create embeddings for all chunks\n",
    "    Model options:\n",
    "    - 'all-MiniLM-L6-v2': Fast, good balance (384 dimensions)\n",
    "    - 'all-mpnet-base-v2': Better quality, slower (768 dimensions)\n",
    "    - 'paraphrase-multilingual-MiniLM-L12-v2': For multilingual content\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading embedding model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Extract text from chunks (adjust based on your JSON structure)\n",
    "    # Common keys: 'text', 'content', 'chunk', 'transcript'\n",
    "    texts = []\n",
    "    for chunk in chunks:\n",
    "        # Try different possible keys\n",
    "        if 'text' in chunk:\n",
    "            texts.append(chunk['text'])\n",
    "        elif 'content' in chunk:\n",
    "            texts.append(chunk['content'])\n",
    "        elif 'chunk' in chunk:\n",
    "            texts.append(chunk['chunk'])\n",
    "        elif isinstance(chunk, str):\n",
    "            texts.append(chunk)\n",
    "        else:\n",
    "            # If none of the above, convert to string\n",
    "            texts.append(str(chunk))\n",
    "    \n",
    "    print(f\"\\nCreating embeddings for {len(texts)} chunks...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    # Create embeddings in batches\n",
    "    embeddings = model.encode(\n",
    "        texts, \n",
    "        show_progress_bar=True, \n",
    "        batch_size=32,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    return embeddings, model, texts\n",
    "\n",
    "def save_database(chunks, embeddings, texts, output_file):\n",
    "    \"\"\"Save chunks, texts, and embeddings to disk\"\"\"\n",
    "    database = {\n",
    "        'chunks': chunks,  # Original chunk objects with metadata\n",
    "        'texts': texts,    # Extracted text strings\n",
    "        'embeddings': embeddings,\n",
    "        'metadata': {\n",
    "            'num_chunks': len(chunks),\n",
    "            'embedding_dim': embeddings.shape[1],\n",
    "            'model': MODEL_NAME\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(database, f)\n",
    "    \n",
    "    file_size_mb = len(pickle.dumps(database)) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATABASE SAVED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "    print(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
    "    print(f\"Approximate file size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline to create embeddings from existing chunks\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CREATING EMBEDDINGS FROM CHUNKS.JSON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load chunks\n",
    "        print(\"\\n[1/3] Loading chunks from JSON...\")\n",
    "        chunks = load_chunks(CHUNKS_FILE)\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"ERROR: No chunks found in the file!\")\n",
    "            return\n",
    "        \n",
    "        # Step 2: Create embeddings\n",
    "        print(\"\\n[2/3] Creating embeddings...\")\n",
    "        embeddings, model, texts = create_embeddings(chunks)\n",
    "        \n",
    "        # Step 3: Save database\n",
    "        print(\"\\n[3/3] Saving database...\")\n",
    "        save_database(chunks, embeddings, texts, OUTPUT_FILE)\n",
    "        \n",
    "        print(\"\\n✓ PROCESSING COMPLETE!\")\n",
    "        print(f\"\\nYou can now use '{OUTPUT_FILE}' for semantic search\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SAMPLE CHUNK:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Text preview: {texts[0][:200]}...\")\n",
    "        print(f\"Embedding shape: {embeddings[0].shape}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nERROR: Could not find '{CHUNKS_FILE}'\")\n",
    "        print(\"Make sure the file is in the same directory as this notebook\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9f92f-a4af-45f4-9de9-83930d68dede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
